import whisper
import sounddevice as sd
import numpy as np
import pyttsx3
import tkinter as tk
from threading import Thread
import webbrowser as web
import pyperclip
import queue
import torch 
import webrtcvad
import collections
import sys
import numpy as np

# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è
DURATION = 3
SAMPLERATE = 32000
CHANNELS = 1

speak_queue = queue.Queue()
model = whisper.load_model("large")  # –ú–æ–∂–Ω–∞ "tiny" –∞–±–æ "small" –¥–ª—è —à–≤–∏–¥–∫–æ—Å—Ç—ñ
engine = pyttsx3.init()
engine.setProperty("rate", 160)
last_text = ""
vad = webrtcvad.Vad(2)  # 0-3
# –í–∏–º–æ–≤–∞ –≥–æ–ª–æ—Å–æ–º
def speak_async():
    while True:
        text = speak_queue.get()
        if text is None:
            break
    print("ü§ñ –í—ñ–¥–ø–æ–≤—ñ–¥—å:", text)
    engine.say(text)
    engine.runAndWait()    
    
# –ó–∞–ø–∏—Å –∞—É–¥—ñ–æ
def record_audio():
    print("üéß –ó–∞–ø–∏—Å—É—î–º–æ...")
    audio = sd.rec(int(DURATION * SAMPLERATE), samplerate=SAMPLERATE, channels=CHANNELS, dtype='float32')
    sd.wait()
    return audio

def frame_generator(audio, frame_duration_ms=30, sample_rate=SAMPLERATE):
    n = int(sample_rate * frame_duration_ms / 1000) * 2 
    offset = 0  
    while offset + n < len(audio):
        yield audio[offset:offset + n]
        offset += n
        
def vad_collector(sample_rate, frame_duration_ms, padding_duration_ms, audio):
    frames = frame_generator(audio, frame_duration_ms, sample_rate)
    vad = webrtcvad.Vad(2)  # 0-3
    padding_frames = int(padding_duration_ms / frame_duration_ms)
    ring_buffer = collections.deque(maxlen=padding_frames)
    triggered = False
    voiced_frames = []

    for frame in frames:
        is_speech = vad.is_speech(frame, sample_rate)
        if not triggered:
            if is_speech:
                triggered = True
                voiced_frames.append(frame)
                ring_buffer.clear()
            else:
                ring_buffer.append(frame)
        else:
            voiced_frames.append(frame)
            if not is_speech:
                ring_buffer.append(frame)
                if len(ring_buffer) == padding_frames:
                    triggered = False
                    yield b''.join(voiced_frames)
                    voiced_frames = []
                    ring_buffer.clear()
    
    if voiced_frames:
        yield b''.join(voiced_frames)

# –†–æ–∑–ø—ñ–∑–Ω–∞–≤–∞–Ω–Ω—è –≥–æ–ª–æ—Å—É
def transcribe(audio):
    print("üß† –†–æ–∑–ø—ñ–∑–Ω–∞—î–º–æ –º–æ–≤–ª–µ–Ω–Ω—è...")
    audio = np.squeeze(audio)
    result = model.transcribe(audio, language="uk")
    return result["text"]

# –û—Å–Ω–æ–≤–Ω–∞ –ª–æ–≥—ñ–∫–∞
def process_audio():
    def task():
        global last_text
        audio = record_audio()
        text = transcribe(audio)
        last_text = text.strip()
        if not text:
            print("‚ùó –ù—ñ—á–æ–≥–æ –Ω–µ —Ä–æ–∑–ø—ñ–∑–Ω–∞–Ω–æ.")
            return    
        print("üó£Ô∏è –°–ø—ñ–≤—Ä–æ–∑–º–æ–≤–Ω–∏–∫ —Å–∫–∞–∑–∞–≤:", text)
        label.config(text=f"üó£Ô∏è {text}")

        # –ú–æ–∂–Ω–∞ —Ç—É—Ç –ø—ñ–¥–∫–ª—é—á–∏—Ç–∏ GPT 
        response = f"–¢–∏ —Å–∫–∞–∑–∞–≤: {text}"
        speak_queue.put(response)     
    
    Thread(target=task, daemon=True).start()
    
    
def copy_and_open_chatgpt():
    if last_text:
        pyperclip.copy(last_text)
        web.open("https://chat.openai.com/")
    else :
        print("‚ùó –ù–µ–º–∞—î —Ç–µ–∫—Å—Ç—É –¥–ª—è –∫–æ–ø—ñ—é–≤–∞–Ω–Ω—è.")

# GUI
def create_gui():
    global label
    root = tk.Tk()
    root.title("üéôÔ∏è –ì–æ–ª–æ—Å–æ–≤–∏–π –∞—Å–∏—Å—Ç–µ–Ω—Ç")
    root.geometry("400x250")

    label = tk.Label(root, text="–ù–∞—Ç–∏—Å–Ω—ñ—Ç—å –∫–Ω–æ–ø–∫—É –¥–ª—è –∑–∞–ø–∏—Å—É –≥–æ–ª–æ—Å—É", wraplength=300)
    label.pack(pady=20)

    record_button = tk.Button(root, text="üéß –ó–∞–ø–∏—Å–∞—Ç–∏", command=process_audio)
    record_button.pack(pady=10)
    
    copy_button = tk.Button(root, text="üìã –ö–æ–ø—ñ—é–≤–∞—Ç–∏ —Ç–µ–∫—Å—Ç", command=copy_and_open_chatgpt)
    copy_button.pack(pady=10)
    
    stop_button = tk.Button(root, text = "‚ùå –í–∏–π—Ç–∏", command=root.quit)
    stop_button.pack(pady=10)
    
    root.mainloop()
    
if __name__ == "__main__":
    Thread(target=speak_async, daemon=True).start()
    create_gui()
